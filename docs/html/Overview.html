<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; pyDarwin 1.1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Algorithms" href="Algorithms.html" />
    <link rel="prev" title="pyDarwin 1.1.0" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> pyDarwin
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">About:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="Algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="Releases.html">Releases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="Examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="Options.html">Options List</a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="Glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="Troubleshooting.html">Troubleshooting</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyDarwin</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="overview">
<span id="starttheory"></span><h1>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h1>
<p>pyDarwin implements a number of machine learning algorithms for model selection.</p>
<p>Machine learning algorithms are broadly divided into two categories:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Supervised learning</p></li>
<li><p>Unsupervised learning</p></li>
</ol>
</div></blockquote>
<p>For supervised learning, the algorithm “learns” to associate certain patterns (e.g., a collection of bitmap pictures) with a set of labeled examples.
For example, if one has 10,000 pictures of cats and dogs (a training set), with each image labeled as “cat” or “dog”, an artificial neural network (ANN)
can find patterns in the 0s and 1s that are associated with “catness” or “dogness”, and can be fairly successful at predicting for any similar set of bitmaps
(a test set) which is a cat and which is a dog.</p>
<p>In contrast, unsupervised learning has no labeled training set. Linear regression is a simple example of supervised learning.
There is an input (X) and an output (Y) and the algorithm identified patterns that match the inputs to the output (intercept and slope(s)). However,
looking for the best independent variables to include in a linear regression model is an unsupervised learning problem, there is no training set of examples
with the “correct” list of independent variables to include.</p>
<p>The traditional model selection/building process for pop PK models is similarly unsupervised. There is no “labeled” training dataset, no collection of datasets
that are known to be 1 compartment, with Volume~WT. Rather, each dataset facilitates a new learning process and the algorithm must discover relationships across different datasets.
In the case of model selection, the inputs (Xs) are the “features” of the model search (not the model, but the model search, e.g., number of compartments, covariates, random effects, etc.)
and the output is some measure of model goodness.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">pyDarwin</span></code>, the model goodness is a user-defined function, with a base of the -2LL output, with user-defined penalties, including parsimony (penalties for estimate parameters), convergence, successful covariance step, plus optional
user-written <code class="docutils literal notranslate"><span class="pre">R</span></code> or <code class="docutils literal notranslate"><span class="pre">Python</span></code> code that can be executed after each run (<a class="reference internal" href="Options.html#use-r-options-desc"><span class="std std-ref">“use_r”</span></a> or <a class="reference internal" href="Options.html#use-python-options-desc"><span class="std std-ref">“use_python”</span></a>). This post run code is useful, for example, if the  user wants to add
a penalty for under or over prediction of Cmax (basically a penalty for <a class="reference external" href="https://link.springer.com/article/10.1023/A:1011555016423">posterior predictive check</a>).</p>
<p>Supervised learning includes algorithms such as regression, artificial neural networks (ANN), decision trees/random forest, and k-nearest neighbor.
Recently, hybrid supervised/unsupervised learning algorithms have been introduced and have proven to be very powerful. The best known of these is Deep Q Network/Reinforcement
Learning (DQN/RL). DQN/RL is a deep neural network (a slightly more complex ANN). However, unlike traditional supervised ANN, there is no training set.
Rather, the method starts with a randomly selected set of weights for nodes in the ANN. Then, based on this random selection, ANN predicts the best model.</p>
<p>At the start, this model will be far from the “true” optimal model. Starting with a single model, however, provides a very small “training set”, and the ANN is now trained on this model.
This process is repeated until the current best predicted model no longer improves. This approach (start with a random representation of the search space, run a few models,
then train the representation) has been adapted to other traditionally supervised methods including Bayesian optimization (Gaussian process - GP),
Random Forest (RF) and gradient boosted random trees (GBRT). These three hybrid algorithms (<a class="reference internal" href="Algorithms.html#gp-desc"><span class="std std-ref">GP</span></a>, <a class="reference internal" href="Algorithms.html#rf-desc"><span class="std std-ref">RF</span></a> , <a class="reference internal" href="Algorithms.html#gbrt-desc"><span class="std std-ref">GBRT</span></a>) have been included in <code class="docutils literal notranslate"><span class="pre">pyDarwin</span></code>’s
algorithm options along with the more traditional Genetic Algorithm (<a class="reference internal" href="Algorithms.html#ga-desc"><span class="std std-ref">GA</span></a>), Particle Swarm Optimization (<a class="reference internal" href="Algorithms.html#pso-desc"><span class="std std-ref">PSO</span></a>) and exhaustive search (<a class="reference internal" href="Algorithms.html#ex-desc"><span class="std std-ref">EX</span></a>).</p>
<p>Traditional PK/PD model selection uses the “downhill method”, starting usually at a trivial model, then adding
“features” (compartments, lag times, nonlinear elimination, covariate effects), and accepting the new model if it is better (“downhill”), based on some user-defined, and somewhat informal criteria.
Typically, this user-defined criteria will include a lower -2LL plus usually a penalty for added parameters plus some other criteria that the user may feel is important. The downhill method is easily the
most efficient method (fewest evaluations of the reward/fitness to reach the convergence) but is highly prone to local minima. However, downhill does play a role in a very efficient
local search, in combination with a global search algorithm (e.g., <a class="reference internal" href="Algorithms.html#ga-desc"><span class="std std-ref">GA</span></a> , <a class="reference internal" href="Algorithms.html#gp-desc"><span class="std std-ref">GP</span></a>, <a class="reference internal" href="Algorithms.html#rf-desc"><span class="std std-ref">RF</span></a> , <a class="reference internal" href="Algorithms.html#gbrt-desc"><span class="std std-ref">GBRT</span></a>, <a class="reference internal" href="Algorithms.html#pso-desc"><span class="std std-ref">PSO</span></a>).</p>
<p>Central to understanding the model selection process (with manual or machine learning), is the concept of the search space. The search space is an n-dimensional
space where each dimension represents a set of mutually exclusive options. That is, there likely will be a dimension for “number of compartments”, with possible
values of 1, 2, or 3. Exactly one of these is required (ignoring the possibility of <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12243">Bayesian model averaging</a>).
Another dimension might be the absorption model, with values of first order, zero order, first order with absorption lag time, etc.). Similarly, candidate
relationships between weight and volume might be: no relationship, linear, or power model. In addition to structural and statistical “features”, other features
of the model, such as initial estimates for parameters, can be searched on. Note that each of these dimensions are discrete, and strictly
categorical (not ordered categorical, i.e., first order isn’t “more than” zero order). With this exception, the model search space is analogous to the
parameter search space used in nonlinear regression. An important difference is that the continuous space in nonlinear
regression has derivatives, and quasi-Newton methods can be used to do a “downhill search” in that space. Please note that quasi-Newton methods are
also at risk of finding local minima, and therefore are sensitive to the initial estimates. In the case of parameter estimation (nonlinear regression), efforts are made to start
the search at a location in the search space near the final estimate, greatly reducing the chance of ending up in a local minimum. No such effort is
made in the traditional downhill model selection method. Rather, the search is usually started at a trivial model, which is likely far from the global minimum.</p>
<p>As the discrete space of model search does not have derivatives, other search methods must be used. The simplest, and the one traditionally used in
model selection, is downhill. While efficient,  it can be demonstrated that this method is not robust <a class="footnote-reference brackets" href="#f1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#f2" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. This lack of robustness is due to
the violation of convexity assumption. That is, the downhill search, in either a continuous space (parameter estimation) or a discrete space (model selection)
assumes that the optimal solution is continuously downhill from every other point in the search space. That is, there are no local minima, you can start anywhere
and you’ll end up in the same place - the global minimum (the results are not sensitive to the “initial estimates”). With this assumption, a covariate will be
“downhill”, regardless of whether tested in a one compartment, two compartment; first order or zero order or any other base model. It doesn’t
matter in what sequence you test hypotheses; it’s all downhill and the answer will be the same. Wade <a class="footnote-reference brackets" href="#f1" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> showed that the results of tests of hypotheses do indeed depend on other
features in the model and Chen <a class="footnote-reference brackets" href="#f2" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> showed that different sequences of tests will commonly yield different final models.</p>
<p>In contrast to the traditional downhill/local search, all algorithms implemented in pyDarwin are global search algorithms that are expected to have a greater
degree of robustness to local minima than downhill search. Note, however, that all search algorithms (except exhaustive search) make assumptions about
the search space. While none of the algorithms in pyDarwin assume convexity, none are completely robust,
and search spaces can be deceptive <a class="footnote-reference brackets" href="#f3" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. For all algorithms, the basic process is the same, start at one or more random models. Then, test those models and learn a little about
the search space to decide which models to test next. The algorithms differ in how they decide which models will be subsequently tested.</p>
<p>While the global search algorithm provides substantial protection from a local minimum in the model search, the global search algorithm is typically not very
good at finding the one or two final changes that result in the best model. This is illustrated in <a class="reference internal" href="Algorithms.html#ga-desc"><span class="std std-ref">Genetic Algorithm</span></a> in that the final change likely
must be made by mutations, a rare event, not by crossover. The solution to this problem is to combine the strength of a global search (robustness to local
minima) with the efficiency of local downhill, or even local exhaustive search. Thus, the global search gets close to the final best solution (much like providing good
initial estimates to NONMEM), and the local search finds the best solution in that local volume of the search space.</p>
<p>The search space is key to the implementation of each algorithm. The overall representation is the same for all algorithms - an n-dimensional discrete search space. The values in each
dimension are then coded into several forms, bit strings and integer string. Ultimately, the model is constructed from the integer string, e.g., values for the number
of compartment dimensions are 1|2|3. However, for GA, this must be coded as bit string. There is one additional representation, referred to as a minimal binary string,
which is used for the local exhaustive step.</p>
<p>The overall process is shown in Figure 1 below:</p>
<blockquote>
<div><figure class="align-default">
<img alt="_images/MLSelection.png" src="_images/MLSelection.png" />
</figure>
</div></blockquote>
<p>The same 3 files are required for any search, whether <a class="reference internal" href="Algorithms.html#ex-desc"><span class="std std-ref">EX</span></a> , <a class="reference internal" href="Algorithms.html#ga-desc"><span class="std std-ref">GA</span></a> , <a class="reference internal" href="Algorithms.html#gp-desc"><span class="std std-ref">GP</span></a>, <a class="reference internal" href="Algorithms.html#rf-desc"><span class="std std-ref">RF</span></a>, <a class="reference internal" href="Algorithms.html#gbrt-desc"><span class="std std-ref">GBRT</span></a>, or <a class="reference internal" href="Algorithms.html#pso-desc"><span class="std std-ref">PSO</span></a>.
These files are described in <a class="reference internal" href="Usage.html#startrequiredfiles"><span class="std std-ref">“Required Files”.</span></a></p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f1" role="note">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Wade JR, Beal SL, Sambol NC. 1994  Interaction between structural, statistical, and covariate models in population pharmacokinetic analysis. J Pharmacokinet Biopharm. 22(2):165-77</p>
</aside>
<aside class="footnote brackets" id="f2" role="note">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>PAGE 30 (2022) Abstr 10091 [<a class="reference external" href="https://www.page-meeting.org/?abstract=10091">https://www.page-meeting.org/?abstract=10091</a>]</p>
</aside>
<aside class="footnote brackets" id="f3" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>PAGE 30 (2022) Abstr 10053 [<a class="reference external" href="https://www.page-meeting.org/default.asp?abstract=10053">https://www.page-meeting.org/default.asp?abstract=10053</a>]</p>
</aside>
</aside>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="pyDarwin 1.1.0" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Algorithms.html" class="btn btn-neutral float-right" title="Algorithms" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Mark Sale.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>