

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Algorithms &mdash; pyDarwin 3.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=0f71c7ad" />

  
      <script src="_static/documentation_options.js?v=dd1205ac"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Support" href="Support.html" />
    <link rel="prev" title="Overview" href="Overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pyDarwin
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">About:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#exhaustive-search">Exhaustive Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="#genetic-algorithm">Genetic Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-process">Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-boosted-random-tree">Gradient Boosted Random Tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="#particle-swarm-optimization-pso">Particle Swarm Optimization (PSO)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-objective-optimization">Multi-Objective Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#moga-nsga-ii">MOGA (NSGA-II)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#moga3-nsga-iii">MOGA3 (NSGA-III)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Support.html">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="Releases.html">Releases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="Examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="Options.html">Options List</a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="Glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="Troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="NLME.html">NLME integration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyDarwin</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Algorithms</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="algorithms">
<span id="the-algorithms"></span><h1>Algorithms<a class="headerlink" href="#algorithms" title="Link to this heading">¶</a></h1>
<p>For problems with larger search spaces, and greater number of model evaluations, <a class="reference internal" href="#ga-desc"><span class="std std-ref">Genetic algorithm</span></a> or <a class="reference internal" href="#rf-desc"><span class="std std-ref">Random Forest</span></a> may
be more appropriate.</p>
<p>Below is a list of recommendations for algorithm selection.</p>
<blockquote>
<div><ul class="simple">
<li><p>Short model run times, large search space (&gt; 100,000 models, expected sample &gt; 1000 models) – <a class="reference internal" href="#ga-desc"><span class="std std-ref">GA</span></a>, <a class="reference internal" href="#rf-desc"><span class="std std-ref">RF</span></a>, <a class="reference internal" href="#gbrt-desc"><span class="std std-ref">GBRT</span></a>, or <a class="reference internal" href="#pso-desc"><span class="std std-ref">PSO</span></a></p></li>
<li><p>Small search space (&lt;100,000, expected # of samples &lt; 1000) - <a class="reference internal" href="#gp-desc"><span class="std std-ref">Gaussian Process</span></a>.</p></li>
<li><p>Very small search space (&lt; 500 models), many cores (&gt; 20) – <a class="reference internal" href="#ex-desc"><span class="std std-ref">Exhaustive Search</span></a>.</p></li>
</ul>
</div></blockquote>
<section id="exhaustive-search">
<span id="ex-desc"></span><h2>Exhaustive Search<a class="headerlink" href="#exhaustive-search" title="Link to this heading">¶</a></h2>
<p>The exhaustive search algorithm is simple to understand. The search space is initially represented as a string of integers - one for each dimension. To facilitate the search,
this integer string is coded into a “minimal binary”. Since all candidate models are executed in an exhaustive search, this algorithm is best suited for small search spaces.</p>
</section>
<section id="genetic-algorithm">
<span id="ga-desc"></span><h2>Genetic Algorithm<a class="headerlink" href="#genetic-algorithm" title="Link to this heading">¶</a></h2>
<p>Genetic Algorithm (GA) is a reproduction of the mathematics of evolution/survival of the fittest. A more detailed discussion <a class="reference external" href="https://en.wikipedia.org/wiki/Genetic_algorithm">on GA can be found here</a>, and
a very readable (but somewhat dated) reference is Genetic Algorithms in Search, Optimization and Machine Learning 13th ed. Edition by David Goldberg. Details of the options (not all of which are available in pyDarwin)
can be found <a class="reference external" href="https://deap.readthedocs.io/en/master/">here</a>.
Briefly, GA presents the search space as a bit string, with each “gene” being a binary number that is decoded into the integer value for that option. For example, for a dimension of Additive vs Additive + proportional
residual error, the integer codes would be:</p>
<ol class="arabic simple">
<li><p>Additive error (e.g., +EPS(1))</p></li>
<li><p>Additive + proportional error (e.g., EXP(EPS(1))+EPS(2))</p></li>
</ol>
<p>It is straightforward enough to code these values [1,2] into a binary [0,1]. For dimensions with more than 2 values, more than 1 bit will be needed. For example, if 1 or 2 or 3 compartments are searched, the
string representation might be:</p>
<ol class="arabic simple">
<li><p>One compartment (ADVAN1)</p></li>
<li><p>Two compartment (ADVAN3)</p></li>
<li><p>Three compartment (ADVAN11)</p></li>
</ol>
<p>and the bit string representation might be:</p>
<ul class="simple">
<li><p>1 - [0,0]</p></li>
<li><p>2 - [0,1] and [1,0]</p></li>
<li><p>3 - [1,1]</p></li>
</ul>
<p>The bit strings for each gene are concatenated into a “chromosome”. The search starts with a population of random bit strings. These bit strings are decoded, and NONMEM control files are constructed from the <a class="reference internal" href="Glossary.html#template"><span class="std std-ref">template file</span></a>
by substituting the selected text from the <a class="reference internal" href="Glossary.html#token-set"><span class="std std-ref">token set</span></a>. The resulting NONMEM control file is run and the <a class="reference internal" href="Glossary.html#fitness"><span class="std std-ref">fitness</span></a> is calculated.
The next generations is created by randomly selecting sets of parent candidates from the population. These parent candidates are then selected based on <a class="reference internal" href="Glossary.html#tournament-selection"><span class="std std-ref">Tournament selection</span></a>.
Once the sets of parents are selected, they undergo crossover and mutation, and a new generation is created. This process is repeated until no further improvement is seen.</p>
</section>
<section id="gaussian-process">
<span id="gp-desc"></span><h2>Gaussian Process<a class="headerlink" href="#gaussian-process" title="Link to this heading">¶</a></h2>
<p>Gaussian Process is one of the two options used in <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_optimization#">Bayesian Optimization</a>. The Gaussian Process specifies the form of the prior and posterior distribution.
Initially the distribution is random, as is the case for all the global search algorithms. Once some models have been run, the distribution can be updated (the “ask” step) and new, more informative samples can be
generated (the “tell” step).</p>
</section>
<section id="random-forest">
<span id="rf-desc"></span><h2>Random Forest<a class="headerlink" href="#random-forest" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Random_forests">Random Forest</a> consists of splitting the search space (based on the “goodness” of each model in this case), thus continuously dividing the
search space into “good” and “bad” regions. As before, the initial divisions are random, but become increasingly well-informed as real values for the fitness/reward of models are
included.</p>
</section>
<section id="gradient-boosted-random-tree">
<span id="gbrt-desc"></span><h2>Gradient Boosted Random Tree<a class="headerlink" href="#gradient-boosted-random-tree" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://towardsdatascience.com/decision-trees-random-forests-and-gradient-boosting-whats-the-difference-ae435cbb67ad">Gradient Boosted Random Tree</a>
is similar to Random Forest, but may increase the precision of the tree building by progressively building the tree and calculating a gradient of the reward/fitness with respect to each decision.</p>
</section>
<section id="particle-swarm-optimization-pso">
<span id="pso-desc"></span><h2>Particle Swarm Optimization (PSO)<a class="headerlink" href="#particle-swarm-optimization-pso" title="Link to this heading">¶</a></h2>
<p>Particle swarm optimization (PSO <a class="footnote-reference brackets" href="#f4" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>) is another approach to optimization that, like Genetic Algorithm,
attempts to reproduce a natural optimization process. In the case of PSO, the natural process is the
swarm behavior of birds and fish, although the specifics of the relationship to bird and fish behavior
is largely speculation. Each particle (candidate NONMEM model in this case) moves through the search
space, as one might imagine individuals in a school of fish or a flock of birds moving together,
but also each bird/fish moving somewhat independently.</p>
<p>The velocity of each particle’s movement is based on two factors:</p>
<ol class="arabic simple">
<li><p>Random movement</p></li>
<li><p>Coordinated movement.</p></li>
</ol>
<p>The coordinated movement is in turn, defined by the following parameters in the <a class="reference internal" href="Options.html#options"><span class="std std-ref">Options List</span></a>:</p>
<ul class="simple">
<li><p><a class="reference internal" href="Options.html#inertia-options-desc"><span class="std std-ref">inertia</span></a> (<span class="math notranslate nohighlight">\(\\w\)</span>): the particle tends to continue moving in the same direction as the previous velocity</p></li>
<li><p><a class="reference internal" href="Options.html#cognitive-options-desc"><span class="std std-ref">cognitive</span></a> (<span class="math notranslate nohighlight">\(c_1\)</span>): the particle tends to move in the direction toward its own best known position</p></li>
<li><p><a class="reference internal" href="Options.html#social-options-desc"><span class="std std-ref">social</span></a> (<span class="math notranslate nohighlight">\(c_2\)</span>): the particle tends to move in the direction toward the current best known position among all particles</p></li>
</ul>
<p>Other parameters for PSO include: <a class="reference internal" href="Options.html#population-size-options-desc"><span class="std std-ref">population_size</span></a>, <a class="reference internal" href="Options.html#neighbor-num-options-desc"><span class="std std-ref">neighbor_number</span></a>,
<a class="reference internal" href="Options.html#p-norm-options-desc"><span class="std std-ref">p_norm</span></a>, and <a class="reference internal" href="Options.html#break-on-no-change-options-desc"><span class="std std-ref">break_on_no_change</span></a>.</p>
<p>As with other optimization algorithms, the downhill step may also be implemented.
The topology defines the region of the swarm whereby individual particles (models in this case) exchange information and thereby act in coordination.
The “star” topology is the only implementation currently available in pyDarwin. The star topology permits particles (i.e., models) to coordinate with a set of nearest neighbors in a
sort of star shape, up to the number of neighbors specified in <a class="reference internal" href="Options.html#neighbor-num-options-desc"><span class="std std-ref">neighbor_number</span></a>.</p>
</section>
<section id="multi-objective-optimization">
<span id="moo-desc"></span><h2>Multi-Objective Optimization<a class="headerlink" href="#multi-objective-optimization" title="Link to this heading">¶</a></h2>
<p>Many model selection problems involve several competing objectives that cannot be reduced to a single scalar value without
introducing arbitrary penalty terms. Multi-objective optimization (MOO <a class="footnote-reference brackets" href="#f5" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>) addresses this by optimizing multiple criteria
simultaneously and identifying a set of non-dominated models that form a Pareto front. A model on the Pareto front cannot be
improved in one objective (for example, objective function value) without worsening at least one other objective (such as model
parsimony or bias in a clinically relevant prediction).</p>
<p>In the context of PopPK model selection, typical objectives include goodness-of-fit (e.g., OFV = minus two times the log-likelihood),
the number of estimated parameters (parsimony), and bias in exposure metrics of interest. Rather than producing a single “best” model,
MOO algorithms in pyDarwin generate a manageable set of non-dominated models that quantify the trade-offs between these objectives.
This set is then presented to the user, who selects one or more preferred models based on both numerical criteria and subjective
considerations such as biological plausibility and diagnostic plots. A detailed case study of applying pyDarwin to PopPK model
selection using these methods is provided in <a class="footnote-reference brackets" href="#f8" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p>
<p>pyDarwin implements multi-objective optimization using genetic algorithms. The currently available algorithms are
MOGA (based on NSGA-II <a class="footnote-reference brackets" href="#f6" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>) and MOGA3 (based on NSGA-III <a class="footnote-reference brackets" href="#f7" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>), described in more detail below. These algorithms are configured
through the <a class="reference internal" href="Options.html#options"><span class="std std-ref">Options List</span></a> (for example, via the <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> option and user-specified objective definitions).</p>
<section id="moga-nsga-ii">
<span id="moga-desc"></span><h3>MOGA (NSGA-II)<a class="headerlink" href="#moga-nsga-ii" title="Link to this heading">¶</a></h3>
<p>MOGA in pyDarwin is based on the Non-dominated Sorting Genetic Algorithm II (NSGA-II <a class="footnote-reference brackets" href="#f6" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>). Like the single-objective
<a class="reference internal" href="#ga-desc"><span class="std std-ref">Genetic Algorithm</span></a>, it represents each candidate model as a chromosome constructed from the <a class="reference internal" href="Glossary.html#template"><span class="std std-ref">template file</span></a>
and <a class="reference internal" href="Glossary.html#token-set"><span class="std std-ref">token set</span></a>, and evolves a population of models over multiple generations. However, instead of optimizing a
single fitness value, NSGA-II ranks models based on multiple objectives and maintains an approximation to the Pareto front at each
generation.</p>
<p>In a typical PopPK application, MOGA is used to simultaneously optimize objective function value (OFV) and the number of estimated
parameters. As generations progress, the front of non-dominated models moves toward regions with better fit and greater parsimony,
making the trade-off between these objectives explicit. A local downhill search step can optionally be combined with the MOGA search,
using a scalar aggregation of the objectives to refine individual models. This combination can identify additional non-dominated
models and may find models with lower OFV at similar levels of parsimony, though it may also highlight models that are more prone to
convergence issues or over-parameterization.</p>
<p>MOGA is selected by setting the <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> option to <code class="docutils literal notranslate"><span class="pre">MOGA</span></code> in the <a class="reference internal" href="Options.html#options"><span class="std std-ref">Options List</span></a>. For this algorithm, the
objectives are fixed to objective function value (OFV) and number of estimated parameters (NEP); users cannot change or extend this
objective set. If users wish to define custom objective functions (with two or more objectives), they should instead select the
<code class="docutils literal notranslate"><span class="pre">MOGA3</span></code> algorithm, and use the provided R or Python post-run code to compute and return those objective values explicitly. In all
cases, pyDarwin returns the resulting set of non-dominated models on the final Pareto front for further evaluation.</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="_images/moga2.png"><img alt="Example MOGA (NSGA-II) two-objective Pareto front" src="_images/moga2.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">Example Pareto front from a MOGA (NSGA-II) search, illustrating the trade-off between objective function value (OFV) and
number of estimated parameters (NEP).</span><a class="headerlink" href="#id10" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="moga3-nsga-iii">
<span id="moga3-desc"></span><h3>MOGA3 (NSGA-III)<a class="headerlink" href="#moga3-nsga-iii" title="Link to this heading">¶</a></h3>
<p>MOGA3 is based on the Non-dominated Sorting Genetic Algorithm III (NSGA-III <a class="footnote-reference brackets" href="#f7" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>), an evolutionary algorithm designed for problems
with three or more objectives. NSGA-III extends NSGA-II by using a reference-point-based non-dominated sorting approach to maintain
diversity along a high-dimensional Pareto front. As with other algorithms in pyDarwin, each candidate model is generated from the
<a class="reference internal" href="Glossary.html#template"><span class="std std-ref">template file</span></a> and <a class="reference internal" href="Glossary.html#token-set"><span class="std std-ref">token set</span></a> and evaluated to obtain the user-defined objectives.</p>
<p>In PopPK model selection, MOGA3 is particularly useful when the user wishes to balance goodness-of-fit, parsimony, and a clinically
relevant prediction metric. For example, objectives might include OFV, total number of estimated parameters, and bias in a steady-state
trough concentration for a key analyte. The resulting Pareto front typically contains models that are better in different respects:
some are more parsimonious, some provide a better fit, and others reduce bias in the clinically important exposure metric.</p>
<p>By exploring these trade-offs, MOGA3 can identify models that perform better on one or more objectives than models obtained with a
traditional single-objective or stepwise approach. As with other algorithms, the final choice among non-dominated models is made by
the pharmacometrician, based on the objectives of the analysis, biological plausibility, and examination of diagnostic graphics.</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="_images/moga3.png"><img alt="Example MOGA3 (NSGA-III) three-objective Pareto front" src="_images/moga3.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">Example MOGA3 (NSGA-III) Pareto front for three objectives: <span class="math notranslate nohighlight">\(f_1\)</span> (OFV), <span class="math notranslate nohighlight">\(f_2\)</span> (NEP), and <span class="math notranslate nohighlight">\(f_3\)</span>, a penalty based
on the difference in <span class="math notranslate nohighlight">\(C_{\max}\)</span> between observed and simulated data, where lower values of <span class="math notranslate nohighlight">\(f_3\)</span> indicate better
agreement (parity) between observed and simulated exposure.</span><a class="headerlink" href="#id11" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p>J. Kennedy and R.C. Eberhart. 1995. Particle Swarm Optimization.
Proceedings of the IEEE International Joint Conference on Neural Networks, 4:1942-1948.</p>
</aside>
<aside class="footnote brackets" id="f5" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Kochenderfer, M. and T. Wheeler. 2019. Algorithms for Optimization. The MIT Press.</p>
</aside>
<aside class="footnote brackets" id="f6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id8">2</a>)</span>
<p>Deb, K., A. Pratap, S. Agarwal and T. Meyarivan. 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2):182-197.</p>
</aside>
<aside class="footnote brackets" id="f7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Deb, K. and H. Jain. 2014. An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints. IEEE Transactions on Evolutionary Computation, 18(4):577-601.</p>
</aside>
<aside class="footnote brackets" id="f8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Li, X., M. Sale, K. Nieforth, J. Craig, F. Wang, D. Solit, K. Feng, M. Hu, R. Bies and L. Zhao. 2024. pyDarwin machine learning algorithms application and comparison in nonlinear mixed-effect model selection and optimization. J Pharmacokinet Pharmacodyn, 51(6):785–796.</p>
</aside>
</aside>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Support.html" class="btn btn-neutral float-right" title="Support" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Mark Sale, Certara.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>